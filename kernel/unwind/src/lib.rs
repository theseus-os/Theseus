//! Support for unwinding the call stack and cleaning up stack frames.
//! 
//! Uses DWARF debugging information (`.eh_frame` and `.gcc_except_table`) from object files.
//! It can also be used to generate stack traces (backtrace) using only that debug information
//! without using frame pointer registers.
//! 
//! The general flow of the unwinding procedure is as follows:
//! * A panic occurs, which jumps to the panic entry point. 
//! * The panic entry point invokes the panic_wrapper, which handles said panic 
//!   by invoking `start_unwinding()` in this crate with the reason for the panic.
//! * `start_unwinding()` generates an iterator over all stack frames in the call stack
//!   (the knowledge for which comes from parsing the .eh_frame section).
//! * `start_unwinding()` creates an unwinding context, which contains the stack frame iterator, 
//!   the reason for the panic, and a reference to the current task being unwound.
//!   It then skips the first several stack frames, which correspond to the panic and unwind handlers themselves.
//!   Note that we cannot unwind those frames because they contain resources that we are currently using for unwinding purposes.
//! * At any point hereafter, the unwinding context must be manually cleaned up.
//! * `start_unwinding()` calls `continue_unwinding()`, which contains the bulk of the unwinding logic.
//! * `continue_unwinding()` iterates to the "next" stack frame (the previous frame in the call stack),
//!   and invokes its cleanup routine (landing pad) if it has one. 
//! * Once the cleanup routine is complete, it jumps to `_Unwind_Resume` automatically. 
//!   This cannot be changed and is an artifact of how unwinding routines are generated by the compiler.
//! * `_Unwind_Resume` is defined alongside the panic entry pointer, and is nothing more
//!   than a simple wrapper that invokes `continue_unwinding()` here. 
//! * `continue_unwinding()` continues iterating up the call stack. 
//!   Once it reaches the end of the call stack (or an error occurs),
//!   we invoke a finalizer routine called `cleanup_unwinding_context()`. 
//! * In `cleanup_unwinding_context()`, the unwinding context pointer is recovered and all unwinding resources are freed.
//!   Finally, the task is marked as killed so it can no longer be scheduled in. 
//! 
//! 
//! The flow of some functions was inspired by gcc's `libunwind`
//! and from `gimli/unwind-rs/src/glue.rs`.

#![no_std]
#![feature(panic_info_message)]
#![feature(asm, naked_functions)]
#![feature(unwind_attributes)]
#![feature(trait_alias)]

extern crate alloc;
#[macro_use] extern crate log;
extern crate memory;
extern crate mod_mgmt;
extern crate irq_safety;
extern crate task;
extern crate gimli;
extern crate fallible_iterator;
extern crate scheduler;
extern crate apic;
extern crate runqueue;
extern crate interrupts;

mod registers;
mod lsda;

use core::fmt;
use alloc::{
    sync::Arc,
    boxed::Box,
};
use gimli::{
    UnwindSection, 
    UnwindTableRow, 
    EhFrame, 
    BaseAddresses, 
    UninitializedUnwindContext, 
    FrameDescriptionEntry,
    Pointer,
    EndianSlice,
    NativeEndian,
    CfaRule,
    RegisterRule,
    X86_64
};
use registers::{Registers, LandingRegisters, SavedRegs};
use fallible_iterator::FallibleIterator;
use mod_mgmt::{
    CrateNamespace,
    metadata::{SectionType, StrongCrateRef, StrongSectionRef},
};
use memory::VirtualAddress;
use task::{TaskRef, KillReason};
use irq_safety::hold_interrupts;


/// This is the context/state that is used during unwinding and passed around
/// to the callback functions in the various unwinding stages, such as in `_Unwind_Resume()`. 
/// 
/// Because those callbacks follow an extern "C" ABI, this structure is passed as a pointer 
/// rather than directly by value or by reference.
/// Thus, it must be manually freed when unwinding is finished (or if it fails in the middle)
/// in order to avoid leaking memory, e.g., not dropping reference counts. 
pub struct UnwindingContext {
    /// The iterator over the current call stack, in which the "next" item in the iterator
    /// is the previous frame in the call stack (the caller frame).
    stack_frame_iter: StackFrameIter,
    /// The reason why we're performing unwinding, which should be set in the panic entry point handler.
    cause: KillReason,
    /// A reference to the current task that is being unwound.
    current_task: TaskRef,
}


/// A single frame in the stack, which contains
/// unwinding-related information for a single function call's stack frame.
/// 
/// See each method for documentation about the members of this struct.
#[derive(Debug)]
pub struct StackFrame {
    personality: Option<u64>,
    lsda: Option<u64>,
    initial_address: u64,
    call_site_address: u64,
}

impl StackFrame {
    /// The address of the personality function that corresponds
    /// to this stack frame's unwinding routine, if needed for this stack frame.
    /// In Rust, this is always the same function, the one defined as the `eh_personality`
    /// language item, something required by the compiler.
    /// 
    /// Note that in Theseus we do not use a personality function,
    /// as we use a custom unwinding flow that bypasses invoking the personality function.
    pub fn personality(&self) -> Option<u64> {
        self.personality
    }

    /// The address of the Language-Specific Data Area (LSDA)
    /// that is needed to discover the unwinding cleanup routines (landing pads)
    /// for this stack frame. 
    /// Typically, this points to an area within the `.gcc_except_table` section,
    /// which then needs to be parsed.
    pub fn lsda(&self) -> Option<u64> {
        self.lsda
    }

    /// The *call site* of this stack frame, i.e.,
    /// the address of the instruction that called the next function in the call stack.
    pub fn call_site_address(&self) -> u64 {
        self.call_site_address
    }

    /// The address (starting instruction pointer) of the function
    /// corresponding to this stack frame. 
    /// This points to the top (entry point) of that function.
    pub fn initial_address(&self) -> u64 {
        self.initial_address
    }
}


/// Essentially a set of crates, useful for symbol resolution.
struct NamespaceContext {
    /// The underlying namespace used to resolve symbols and find sections from addresses.
    namespace: Arc<CrateNamespace>, 
    /// The additional crate (usually an application crate) that is not
    /// a member of the `namespace` but should be used for additional symbol resolution.
    starting_crate: Option<StrongCrateRef>,
}


/// An iterator over all of the stack frames on the current stack,
/// which works in reverse calling order from the current function
/// up the call stack to the very first function on the stack,
/// at which point it will return `None`. 
/// 
/// This is a lazy iterator: the previous frame in the call stack
/// is only calculated upon invocation of the `next()` method. 
/// 
/// This can be used with the `FallibleIterator` trait.
pub struct StackFrameIter {
    /// A reference to the underlying namespace crates 
    /// that are used to resolve symbols and section addresses 
    /// when iterating over stack frames. 
    namespace_context: NamespaceContext,
    /// The register values that 
    /// These register values will change on each invocation of `next()`
    /// as different stack frames are successively iterated over.
    registers: Registers,
    /// Unwinding state related to the previous frame in the call stack:
    /// a reference to its row/entry in the unwinding table,
    /// and the Canonical Frame Address (CFA value) that is used to determine the next frame.
    state: Option<(UnwindRowReference, u64)>,
    /// An extra offset that is used to augment the calculation of register rule values
    /// in weird circumstances, such as when unwinding from an exception stack frame `B` 
    /// to a frame `A` that caused the exception, even though frame `A` did not "call" frame `B`.
    extra_cfa_offset: Option<i64>,

    last_frame_was_exception_handler: bool,
}

impl fmt::Debug for StackFrameIter {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        // write!(f, "StackFrameIter {{\nRegisters: {:?},\nstate: {:#X?}\n}}", self.registers, self.state)
        write!(f, "StackFrameIter {{\nRegisters: {:?}\n}}", self.registers)
    }
}

impl StackFrameIter {
    /// Create a new iterator over stack frames that starts from the current frame
    /// and uses the given `Registers` values as a starting point. 
    /// The given `namespace` and `starting_crate` are used for resolving symbol addresses into sections.
    /// 
    /// Note: ideally, this shouldn't be public since it needs to be invoked with the correct initial register values.
    #[doc(hidden)]
    pub fn new(namespace: Arc<CrateNamespace>, app_crate: Option<StrongCrateRef>, registers: Registers) -> Self {
        StackFrameIter {
            namespace_context: NamespaceContext { namespace, starting_crate: app_crate },
            registers,
            state: None,
            extra_cfa_offset: None,
            last_frame_was_exception_handler: false,
        }
    }

    /// Returns the array of register values as they existed during the stack frame
    /// that is currently being iterated over. 
    /// This is necessary in order to restore the proper register values 
    /// before jumping to the **landing pad** (a cleanup function or exception catcher/panic handler)
    /// such that the landing pad function will actually execute properly with the right context.
    pub fn registers(&self) -> &Registers {
        &self.registers
    }

    /// Returns a reference to the underlying `CrateNamespace`
    /// that is used for symbol resolution while iterating over these stack frames.
    pub fn namespace(&self) -> &Arc<CrateNamespace> {
        &self.namespace_context.namespace
    }

    /// Returns a reference to the additional crate (usually an application crate) that is not
    /// a member of the underlying `namespace`, but should also be used for symbol resolution.
    pub fn starting_crate(&self) -> Option<&StrongCrateRef> {
        self.namespace_context.starting_crate.as_ref()
    }
}

impl FallibleIterator for StackFrameIter {
    type Item = StackFrame;
    type Error = &'static str;

    fn next(&mut self) -> Result<Option<Self::Item>, Self::Error> {
        let registers = &mut self.registers;
        let prev_cfa_offset = self.extra_cfa_offset;
        let last_frame_was_exception_handler = self.last_frame_was_exception_handler;

        if let Some((unwind_row_ref, cfa)) = self.state.take() {
            let mut newregs = registers.clone();
            newregs[X86_64::RA] = None;

            unwind_row_ref.with_unwind_info(|_fde, row| {
                // There is some strange behavior when moving up the call stack 
                // from an exception handler function's frame `B` to a frame `A` that resulted in the exception,
                // since frame `A` did not call frame `B` directly, 
                // and since the CPU may have pushed an error code onto the stack,
                // which messes up the DWARF info that calculates register values properly. 
                //
                // In this case, the `cfa` value must be modified to account for that error code 
                // being pushed onto the stack (by adding an offset of `8` to the `cfa` value),
                // and also the register rule calculations will be misaligned as follows:
                // each rule calculates a resultant value that applies to the *next* register number in the list.
                // Thus, we save the previous resultant value for use in the next iteration. 
                //
                // Note that the first register value is calculated using the same `cfa` offset (`8`)
                // as an additional offset from the register rule's offset. I'm not sure whether it's 
                // always the first register value, or the return address `RA` value that uses that extra offset,
                // but so far in every case I've observed, the rule for the return address `RA` is first.
                let mut prev_value: u64 = 0;
                for (index, &(reg_num, ref rule)) in row.registers().enumerate() {
                    debug!("Looking at register rule:  {:?} {:?}", reg_num, rule);
                    // The stack pointer (RSP) is given by the CFA calculated during the previous iteration;
                    // there should *not* be a register rule defining the value of the RSP directly.
                    if reg_num == X86_64::RSP {
                        return Err("BUG: a unwind row's register rule specified that RSP should be changed, which is invalid.");
                    }
                    newregs[reg_num] = match *rule {
                        RegisterRule::Undefined => return Err("StackFrameIter: encountered an unsupported RegisterRule::Undefined"), // registers[reg_num],
                        RegisterRule::SameValue => registers[reg_num],
                        RegisterRule::Register(other_reg_num) => registers[other_reg_num],
                        // This is the most common register rule (in fact, the only one we've seen),
                        // so we may have to adapt the logic herein for use in other rules. 
                        RegisterRule::Offset(offset) => {
                            let normal_value = unsafe { *(cfa.wrapping_add(offset as u64) as *const u64) };
                            trace!("     cfa: {:#X}, addr: {:#X}, normal_value: {:#X}", cfa, cfa.wrapping_add(offset as u64), normal_value);
                            if let Some(extra_offset) = prev_cfa_offset {
                                let reg_value = if index == 0 {
                                    let value_extra_offset = unsafe { *(cfa.wrapping_add((offset + extra_offset) as u64) as *const u64) };
                                    trace!("     cfa: {:#X}, addr: {:#X}, extra_value: {:#X}", cfa, cfa.wrapping_add((offset + extra_offset) as u64), value_extra_offset);
                                    value_extra_offset
                                } else {
                                    prev_value
                                };
                                prev_value = normal_value;

                                trace!("     cfa: {:#X}, RETURNED value: {:#X}", cfa, reg_value);
                                Some(reg_value)
                            } else {
                                // this is the regular case
                                Some(normal_value)
                            }
                            // let addrO = (cfa.wrapping_add(offset as u64));
                            // let valO = unsafe { *(addrO as *const u64) };
                            // trace!("     cfa: {:#X}, addrO: {:#X}, valO: {:#X}", cfa, addrO, valO);
                            // let addrN = (cfa.wrapping_add(offset + extra_offset as u64));
                            // let valN = unsafe { *(addrN as *const u64) };
                            // trace!("     cfa: {:#X}, addrN: {:#X}, valN: {:#X}", cfa, addrN, valN);
                            // Some(valO)
                        }
                        RegisterRule::ValOffset(offset) => Some(cfa.wrapping_add(offset as u64)),
                        RegisterRule::Expression(_) => return Err("StackFrameIter: encountered an unsupported RegisterRule::Expression"),
                        RegisterRule::ValExpression(_) => return Err("StackFrameIter: encountered an unsupported RegisterRule::ValExpression"),
                        RegisterRule::Architectural => return Err("StackFrameIter: encountered an unsupported RegisterRule::Architectural"),
                    };
                }
                Ok(())
            })?;

            // set the stack pointer according to the previously-calculated CFA 
            newregs[X86_64::RSP] = Some(cfa);
            *registers = newregs;
        }


        if let Some(return_address) = registers[X86_64::RA] {
            // we've reached the end of the stack, so we're done iterating
            if return_address == 0 {
                return Ok(None);
            }

            // The return address (RA register) points to the next instruction (1 byte past the call instruction),
            // since the processor has advanced it to the next instruction to continue executing after the function returns. 
            // As x86 has variable-length instructions, we don't know exactly where the previous instruction starts,
            // but we know that subtracting one will give us an address within that previous instruction.
            let caller = return_address - 1;

            // Get unwind info for the call site address
            let crate_ref = self.namespace_context.namespace.get_crate_containing_address(
                VirtualAddress::new_canonical(caller as usize), 
                self.namespace_context.starting_crate.as_ref(),
                false,
            ).ok_or_else(|| {
                error!("StackTraceIter::next(): couldn't get crate containing call site address: {:#X}", caller);
                "couldn't get crate containing call site address"
            })?;
            let (eh_frame_sec_ref, base_addrs) = get_eh_frame_info(&crate_ref)
                .ok_or("couldn't get eh_frame section in caller's containing crate")?;

            let mut extra_cfa_offset: Option<i64> = None;
            let mut this_frame_is_exception_handler = false;
            let row_ref = UnwindRowReference { caller, eh_frame_sec_ref, base_addrs };
            let (cfa, frame) = row_ref.with_unwind_info(|fde, row| {
                // trace!("ok: {:?} (0x{:x} - 0x{:x})", row.cfa(), row.start_address(), row.end_address());
                let mut cfa = match *row.cfa() {
                    CfaRule::RegisterAndOffset { register, offset } => {
                        debug!("CfaRule:RegisterAndOffset: reg {:?}, offset: {:#X}", register, offset);
                        let reg_value = registers[register].ok_or_else(|| {
                            error!("CFA rule specified register {:?} with offset {:#X}, but register {:?}({}) had no value!", register, offset, register, register.0);
                            "CFA rule specified register with offset, but that register had no value."
                        })?;
                        reg_value.wrapping_add(offset as u64)
                    }
                    CfaRule::Expression(_expr) => {
                        error!("CFA rules based on Expressions are not yet supported. Expression: {:?}", _expr);
                        return Err("CFA rules based on Expressions are not yet supported.");
                    }
                };
                
                trace!("initial_address: {:#X}", fde.initial_address());
                extra_cfa_offset = if interrupts::is_exception_handler_with_error_code(fde.initial_address()) {
                    warn!("FOUND EXCEPTION HANDLER WITH ERROR CODE!");
                    let cfa_offset: i64 = core::mem::size_of::<usize>() as i64;
                    cfa = cfa.wrapping_add(cfa_offset as u64);
                    this_frame_is_exception_handler = true;
                    Some(cfa_offset)
                    // None
                } else {
                    None
                };

                if last_frame_was_exception_handler { 
                    cfa = cfa.wrapping_add(5 * 8); // size of `ExceptionStackFrame`
                    warn!("LAST FRAME was exception handler: adding {:#X} to cfa, new cfa: {:#X}", 5*8, cfa);
                }

                trace!("cfa is {:#X}", cfa);
                trace!("call_site_address: {:#X}", caller);

                let frame = StackFrame {
                    personality: fde.personality().map(|x| unsafe { deref_ptr(x) }),
                    lsda: fde.lsda().map(|x| unsafe { deref_ptr(x) }),
                    initial_address: fde.initial_address(),
                    call_site_address: caller,
                };
                Ok((cfa, frame))
            })?;

            // since we can't double-borrow `self` in the above closure, we assign its state(s) here.
            self.extra_cfa_offset = extra_cfa_offset;
            self.last_frame_was_exception_handler = this_frame_is_exception_handler;
            self.state = Some((row_ref, cfa));
            Ok(Some(frame))
        } else {
            Ok(None)
        }
    }
}


/// Dereferences a `Pointer` type found in unwinding information,
/// which is either direct (no dereference) or indirect. 
/// Doing so is unsafe because the value of the `Pointer` is not checked. 
unsafe fn deref_ptr(ptr: Pointer) -> u64 {
    match ptr {
        Pointer::Direct(x) => x,
        Pointer::Indirect(x) => *(x as *const u64),
    }
}


pub trait FuncWithRegisters = Fn(Registers) -> Result<(), &'static str>;
type RefFuncWithRegisters<'a> = &'a dyn FuncWithRegisters;


/// This function saves the current CPU register values onto the stack (to preserve them)
/// and then invokes the given closure with those registers as the argument.
/// 
/// In general, this is useful for jumpstarting the unwinding procedure,
/// since we have to start from the current call frame and work backwards up the call stack 
/// while applying the rules for register value changes in each call frame
/// in order to arrive at the proper register values for a prior call frame.
pub fn invoke_with_current_registers<F>(f: F) -> Result<(), &'static str> 
    where F: FuncWithRegisters 
{
    let f: RefFuncWithRegisters = &f;
    let result = unsafe { 
        let res_ptr = unwind_trampoline(&f);
        let res_boxed = Box::from_raw(res_ptr);
        *res_boxed
    };
    return result;
    // this is the end of the code in this function, the following is just inner functions.


    /// This is an internal assembly function used by `invoke_with_current_registers()` 
    /// that saves the current register values by pushing them onto the stack
    /// before invoking the function "unwind_recorder" with those register values as the only argument.
    /// This is needed because the unwind info tables describe register values as operations (offsets/addends)
    /// that are relative to the current register values, so we must have those current values as a starting point.
    /// 
    /// The argument is a pointer to a function reference, so effectively a pointer to a pointer. 
    #[naked]
    #[inline(never)]
    unsafe extern "C" fn unwind_trampoline(_func: *const RefFuncWithRegisters) -> *mut Result<(), &'static str> {
        // This is a naked function, so you CANNOT place anything here before the asm block, not even log statements.
        // This is because we rely on the value of registers to stay the same as whatever the caller set them to.
        // DO NOT touch RDI register, which has the `_func` function; it needs to be passed into unwind_recorder.
        asm!("
            # copy the stack pointer to RSI
            movq %rsp, %rsi
            pushq %rbp
            pushq %rbx
            pushq %r12
            pushq %r13
            pushq %r14
            pushq %r15
            # To invoke `unwind_recorder`, we need to put: 
            # (1) the func in RDI (it's already there, just don't overwrite it),
            # (2) the stack in RSI,
            # (3) a pointer to the saved registers in RDX.
            movq %rsp, %rdx   # pointer to saved regs (on the stack)
            call unwind_recorder
            # restore saved registers
            popq %r15
            popq %r14
            popq %r13
            popq %r12
            popq %rbx
            popq %rbp
            ret
        ");
        core::hint::unreachable_unchecked();
    }


    /// The calling convention dictates the following order of arguments: 
    /// * first arg in `RDI` register, the function (or closure) to invoke with the saved registers arg,
    /// * second arg in `RSI` register, the stack pointer,
    /// * third arg in `RDX` register, the saved register values used to recover execution context
    ///   after we change the register values during unwinding,
    #[no_mangle]
    unsafe extern "C" fn unwind_recorder(
        func: *const RefFuncWithRegisters,
        stack: u64,
        saved_regs: *mut SavedRegs,
    ) -> *mut Result<(), &'static str> {
        let func = &*func;
        let saved_regs = &*saved_regs;

        let mut registers = Registers::default();
        registers[X86_64::RBX] = Some(saved_regs.rbx);
        registers[X86_64::RBP] = Some(saved_regs.rbp);
        registers[X86_64::RSP] = Some(stack + 8); // the stack value passed in is one pointer width before the real RSP
        registers[X86_64::R12] = Some(saved_regs.r12);
        registers[X86_64::R13] = Some(saved_regs.r13);
        registers[X86_64::R14] = Some(saved_regs.r14);
        registers[X86_64::R15] = Some(saved_regs.r15);
        registers[X86_64::RA]  = Some(*(stack as *const u64));

        let res = func(registers);
        Box::into_raw(Box::new(res))
    }
}


/// **Landing** refers to the process of jumping to a handler for a stack frame,
/// e.g., an unwinding cleanup function, or an exception "catch" block.
/// 
/// This function basically fills the actual CPU registers with the values in the given `LandingRegisters`
/// and then jumps to the exception handler (landing pad) pointed to by the stack pointer (RSP) in those `LandingRegisters`.
/// 
/// This is similar in design to how the latter half of a context switch routine
/// must restore the previously-saved registers for the next task.
unsafe fn land(regs: &Registers, landing_pad_address: u64) -> Result<(), &'static str> {
    let mut landing_regs = LandingRegisters {
        rax: regs[X86_64::RAX].unwrap_or(0),
        rbx: regs[X86_64::RBX].unwrap_or(0),
        rcx: regs[X86_64::RCX].unwrap_or(0),
        rdx: regs[X86_64::RDX].unwrap_or(0),
        rdi: regs[X86_64::RDI].unwrap_or(0),
        rsi: regs[X86_64::RSI].unwrap_or(0),
        rbp: regs[X86_64::RBP].unwrap_or(0),
        r8:  regs[X86_64::R8 ].unwrap_or(0),
        r9:  regs[X86_64::R9 ].unwrap_or(0),
        r10: regs[X86_64::R10].unwrap_or(0),
        r11: regs[X86_64::R11].unwrap_or(0),
        r12: regs[X86_64::R12].unwrap_or(0),
        r13: regs[X86_64::R13].unwrap_or(0),
        r14: regs[X86_64::R14].unwrap_or(0),
        r15: regs[X86_64::R15].unwrap_or(0),
        rsp: regs[X86_64::RSP].ok_or("unwind::land(): RSP was None, \
            it must be set so that the landing pad function can execute properly."
        )?,
    };

    // Now place the landing pad function's address at the "bottom" of the stack
    // -- not really the bottom of the whole stack, just the last thing to be popped off after the landing_regs.
    landing_regs.rsp -= core::mem::size_of::<u64>() as u64;
    *(landing_regs.rsp as *mut u64) = landing_pad_address;
    // trace!("unwind_lander regs: {:#X?}", landing_regs);
    unwind_lander(&landing_regs);
    // this is the end of the code in this function, the following is just inner functions.


    /// This function places the values of the given landing registers
    /// into the actual CPU registers, and then jumps to the landing pad address
    /// specified by the stack pointer in those registers. 
    /// 
    /// It is marked as divergent (returning `!`) because it doesn't return to the caller,
    /// instead it returns (jumps to) that landing pad address.
    #[naked]
    #[inline(never)]
    unsafe extern fn unwind_lander(_regs: *const LandingRegisters) -> !{
        asm!("
            movq %rdi, %rsp
            popq %rax
            popq %rbx
            popq %rcx
            popq %rdx
            popq %rdi
            popq %rsi
            popq %rbp
            popq %r8
            popq %r9
            popq %r10
            popq %r11
            popq %r12
            popq %r13
            popq %r14
            popq %r15
            movq 0(%rsp), %rsp
            # now we jump to the actual landing pad function
            ret
        ");
        core::hint::unreachable_unchecked();
    }
}


type NativeEndianSliceReader<'i> = EndianSlice<'i, NativeEndian>;


/// Due to lifetime and locking issues, we cannot store a direct reference to an unwind table row. 
/// Instead, here we store references to the objects needed to calculate/obtain an unwind table row.
#[derive(Debug)]
struct UnwindRowReference {
    caller: u64,
    eh_frame_sec_ref: StrongSectionRef,
    base_addrs: BaseAddresses,
}
impl UnwindRowReference {
    /// Accepts a closure/function that will be invoked with following unwinding information:
    /// a frame description entry and an unwinding table row.
    fn with_unwind_info<O, F>(&self, mut f: F) -> Result<O, &'static str>
        where F: FnMut(&FrameDescriptionEntry<NativeEndianSliceReader, usize>, &UnwindTableRow<NativeEndianSliceReader>) -> Result<O, &'static str>
    {
        let sec = self.eh_frame_sec_ref.lock();
        let size_in_bytes = sec.size();
        let sec_pages = sec.mapped_pages.lock();
        let eh_frame_slice: &[u8] = sec_pages.as_slice(sec.mapped_pages_offset, size_in_bytes)?;
        let eh_frame = EhFrame::new(eh_frame_slice, NativeEndian);
        let mut unwind_ctx = UninitializedUnwindContext::new();
        let fde = eh_frame.fde_for_address(&self.base_addrs, self.caller, EhFrame::cie_from_offset).map_err(|_e| {
            error!("gimli error: {:?}", _e);
            "gimli error while finding FDE for address"
        })?;
        let unwind_table_row = fde.unwind_info_for_address(&eh_frame, &self.base_addrs, &mut unwind_ctx, self.caller).map_err(|_e| {
            error!("gimli error: {:?}", _e);
            "gimli error while finding unwind info for address"
        })?;
        
        // debug!("FDE: {:?} ", fde);
        // let mut instructions = fde.instructions(&eh_frame, &self.base_addrs);
        // while let Some(instr) = instructions.next().map_err(|_e| {
        //     error!("FDE instructions gimli error: {:?}", _e);
        //     "gimli error while iterating through eh_frame FDE instructions list"
        // })? {
        //     debug!("    FDE instr: {:?}", instr);
        // }

        f(&fde, &unwind_table_row)
    }
}


/// Returns a tuple of .eh_frame section for the given `crate_ref`
/// and the base addresses (its .text section address and .eh_frame section address).
/// 
/// # Locking / Deadlock
/// Obtains the lock on the given `crate_ref` 
/// and the lock on all of its sections while iterating through them.
/// 
/// The latter lock on the crate's `rodata_pages` object will be held
/// for the entire lifetime of the returned object. 
fn get_eh_frame_info(crate_ref: &StrongCrateRef) -> Option<(StrongSectionRef, BaseAddresses)> {
    let parent_crate = crate_ref.lock_as_ref();

    let eh_frame_sec_ref = parent_crate.sections.values()
        .filter(|s| s.lock().typ == SectionType::EhFrame)
        .next()?;
    
    let eh_frame_vaddr = eh_frame_sec_ref.lock().start_address().value();
    let text_pages_vaddr = parent_crate.text_pages.as_ref()?.1.start.value();
    let base_addrs = BaseAddresses::default()
        .set_eh_frame(eh_frame_vaddr as u64)
        .set_text(text_pages_vaddr as u64);

    Some((eh_frame_sec_ref.clone(), base_addrs))
}


// /// Print a stack trace without using frame pointers. 
// /// This should always work regardless of compiler flags 
// /// because it uses the DWARF information from the .eh_frame section
// /// to understand the layout and content of each stack frame.
// /// 
// /// If the object file containing a given symbol has been fully stripped,
// /// then this will not be able to resolve any symbols that originate from that object file.
// pub fn print_stack_frames(stack_frames: &mut StackFrameIter) -> Result<(), &'static str> {
//     while let Some(frame) = stack_frames.next()? {
//         info!("StackFrame: {:#X?}", frame);
//         info!("  in func: {:?}", stack_frames.namespace_context.namespace.get_section_containing_address(VirtualAddress::new_canonical(frame.initial_address() as usize), stack_frames.namespace_context.starting_crate, false));
//         if let Some(lsda) = frame.lsda() {
//             info!("  LSDA section: {:?}", stack_frames.namespace_context.namespace.get_section_containing_address(VirtualAddress::new_canonical(lsda as usize), stack_frames.namespace_context.starting_crate, true));
//         }
//     }
// }


/// Starts the unwinding procedure for the current task 
/// by working backwards up the call stack starting from the current stack frame.
/// 
/// # Arguments
/// * `reason`: the reason why the current task is being killed, e.g., due to a panic, exception, etc.
/// * `stack_frames_to_skip`: the number of stack frames that should be skipped in order to avoid unwinding them.
///   For example, for a panic, the first `5` frames in the call stack can be ignored.
/// 
#[doc(hidden)]
pub fn start_unwinding(reason: KillReason, stack_frames_to_skip: usize) -> Result<(), &'static str> {
    // Here we have to be careful to have no resources waiting to be dropped/freed/released on the stack. 
    let unwinding_context_ptr = {
        let curr_task = task::get_my_current_task().ok_or("get_my_current_task() failed")?;
        let namespace = curr_task.get_namespace();
        let app_crate_ref = { 
            let t = curr_task.lock();
            t.app_crate.as_ref().map(|a| a.clone_shallow())
        };

        Box::into_raw(Box::new(
            UnwindingContext {
                stack_frame_iter: StackFrameIter::new(
                    namespace,
                    app_crate_ref,
                    // we will set the real register values later, in the `invoke_with_current_registers()` closure.
                    Registers::default()
                ), 
                cause: reason,
                current_task: curr_task.clone(),
            }
        ))
    };

    // IMPORTANT NOTE!!!!
    // From this point on, if there is a failure, we need to free the unwinding context pointer to avoid leaking things.


    // We pass a pointer to the unwinding context to this closure. 
    let res = invoke_with_current_registers(|registers| {
        // set the proper register values before we used the 
        {  
            // SAFE: we just created this pointer above
            let unwinding_context = unsafe { &mut *unwinding_context_ptr };
            unwinding_context.stack_frame_iter.registers = registers;
            
            // Skip the first several frames, e.g., to skip unwinding functions in the panic handlers themselves.
            for _i in 0..stack_frames_to_skip {
                unwinding_context.stack_frame_iter.next()
                    .map_err(|_e| {
                        error!("error skipping call stack frame {} in unwinder", _i);
                        "error skipping call stack frame in unwinder"
                    })?
                    .ok_or("call stack frame did not exist (we were trying to skip it)")?;
            }
        }

        continue_unwinding(unwinding_context_ptr)
    });

    match &res {
        &Ok(()) => {
            debug!("unwinding procedure has reached the end of the stack.");
        }
        &Err(e) => {
            error!("BUG: unwinding the first stack frame returned unexpectedly. Error: {}", e);
        }
    }
    cleanup_unwinding_context(unwinding_context_ptr);
}


/// Continues the unwinding process from the point it left off at, 
/// which is defined by the given unwinding context.
/// 
/// This returns an error upon failure, 
/// and an `Ok(())` when it reaches the end of the stack and there are no more frames to unwind.
/// When either value is returned (upon a return of any kind),
/// **the caller is responsible for cleaning up the given `UnwindingContext`.
/// 
/// Upon successfully continuing to iterate up the call stack, this function will actually not return at all. 
fn continue_unwinding(unwinding_context_ptr: *mut UnwindingContext) -> Result<(), &'static str> {
    let stack_frame_iter = unsafe { &mut (*unwinding_context_ptr).stack_frame_iter };
    
    trace!("continue_unwinding(): stack_frame_iter: {:#X?}", stack_frame_iter);
    
    let (mut regs, landing_pad_address) = if let Some(frame) = stack_frame_iter.next().map_err(|e| {
        error!("continue_unwinding: error getting next stack frame in the call stack: {}", e);
        "continue_unwinding: error getting next stack frame in the call stack"
    })? {
        info!("Unwinding StackFrame: {:#X?}", frame);
        info!("  In func: {:?}", stack_frame_iter.namespace().get_section_containing_address(VirtualAddress::new_canonical(frame.initial_address() as usize), stack_frame_iter.starting_crate(), false));
        info!("  Regs: {:?}", stack_frame_iter.registers());

        if let Some(lsda) = frame.lsda() {
            let lsda = VirtualAddress::new_canonical(lsda as usize);
            if let Some((lsda_sec_ref, _)) = stack_frame_iter.namespace().get_section_containing_address(lsda, stack_frame_iter.starting_crate(), true) {
                info!("  parsing LSDA section: {:?}", lsda_sec_ref);
                let sec = lsda_sec_ref.lock();
                let starting_offset = sec.mapped_pages_offset + (lsda.value() - sec.address_range.start.value());
                let length_til_end_of_mp = sec.address_range.end.value() - lsda.value();
                let sec_mp = sec.mapped_pages.lock();
                let lsda_slice = sec_mp.as_slice::<u8>(starting_offset, length_til_end_of_mp)
                    .map_err(|_e| "continue_unwinding(): couldn't get LSDA pointer as a slice")?;
                let table = lsda::GccExceptTableArea::new(lsda_slice, NativeEndian, frame.initial_address());

                let entry = table.call_site_table_entry_for_address(frame.call_site_address()).map_err(|e| {
                    error!("continue_unwinding(): couldn't find a call site table entry for this stack frame's call site address. Error: {}", e);
                    "continue_unwinding(): couldn't find a call site table entry for this stack frame's call site address."
                })?;

                debug!("Found call site entry for address {:#X}: {:#X?}", frame.call_site_address(), entry);
                (stack_frame_iter.registers().clone(), entry.landing_pad_address())
            } else {
                error!("  BUG: couldn't find LSDA section (.gcc_except_table) for LSDA address: {:#X}", lsda);
                return Err("BUG: couldn't find LSDA section (.gcc_except_table) for LSDA address specified in stack frame");
            }
        } else {
            trace!("continue_unwinding(): stack frame has no LSDA");
            return continue_unwinding(unwinding_context_ptr);
        }
    } else {
        trace!("continue_unwinding(): NO REMAINING STACK FRAMES");
        return Ok(());
    };

    // Jump to the actual landing pad function, or rather, a function that will jump there after setting up register values properly.
    debug!("Jumping to landing pad (cleanup function) at {:#X}", landing_pad_address);
    // Once the unwinding cleanup function is done, it will call _Unwind_Resume (technically, it jumps to it),
    // and pass the value in the landing registers' RAX register as the argument to _Unwind_Resume. 
    // So, whatever we put into RAX in the landing regs will be placed into the first arg (RDI) in _Unwind_Resume.
    // This is arch-specific; for x86_64 the transfer is from RAX -> RDI, for ARM/AARCH64, the transfer is from R0 -> R1 or X0 -> X1.
    // See this for more mappings: <https://github.com/rust-lang/rust/blob/master/src/libpanic_unwind/gcc.rs#L102>
    regs[gimli::X86_64::RAX] = Some(unwinding_context_ptr as u64);
    unsafe {
        land(&regs, landing_pad_address)?;
    }
    error!("BUG: call to unwind::land() returned, which should never happen!");
    Err("BUG: call to unwind::land() returned, which should never happen!")
}


/// This function is invoked after each unwinding cleanup routine has finished.
/// Thus, this is a middle point in the unwinding execution flow; 
/// here we need to continue (*resume*) the unwinding procedure 
/// by basically figuring out where we just came from and picking up where we left off. 
#[doc(hidden)]
pub unsafe fn unwind_resume(unwinding_context_ptr: usize) -> ! {
    // trace!("unwind_resume(): unwinding_context_ptr value: {:#X}", unwinding_context_ptr);
    let unwinding_context_ptr = unwinding_context_ptr as *mut UnwindingContext;

    match continue_unwinding(unwinding_context_ptr) {
        Ok(()) => {
            debug!("unwind_resume(): continue_unwinding() returned Ok(), meaning it's at the end of the call stack.");
        }
        Err(e) => {
            error!("BUG: in unwind_resume(): continue_unwinding() returned an error: {}", e);
        }
    }
    // here, cleanup the unwinding state and kill the task
    cleanup_unwinding_context(unwinding_context_ptr);
}


/// This function should be invoked when the unwinding procedure is finished, or cannot be continued any further.
/// It cleans up the `UnwindingContext` object pointed to by the given pointer and marks the current task as killed.
fn cleanup_unwinding_context(unwinding_context_ptr: *mut UnwindingContext) -> ! {

    // Just like in `task_wrapper`, these functions need to be run atomically (without preemption)
    // to ensure that they all get fully executed even after the task is killed.
    // Here: now that the task is finished being unwound and cleaned up, we must do three things:
    // 1. Put the task into a non-runnable mode (killed), and set its kill reason
    // 2. Remove it from its runqueue
    // 3. Yield the CPU
    // The first two need to be done "atomically" (without interruption), so we must disable preemption before step 1.
    // Otherwise, this task could be marked as `Killed`, and then a context switch could occur to another task,
    // which would prevent this task from ever running again, so it would never get to remove itself from the runqueue.
    {
        // (0) disable preemption (currently just disabling interrupts altogether)
        let _held_interrupts = hold_interrupts();

        {
            // Recover ownership of the unwinding context from its pointer
            let unwinding_context_boxed = unsafe { Box::from_raw(unwinding_context_ptr) };
            let unwinding_context = *unwinding_context_boxed;
            let UnwindingContext { current_task, cause, .. } = unwinding_context;

            // (1) Put the task into a non-runnable mode (killed), and set its kill reason
            match current_task.mark_as_killed(cause) {
                Ok(()) => {
                    debug!("cleanup_unwinding_context(): marked task as killed, {:?}", current_task);
                }
                Err(e) => {
                    error!("BUG: in cleanup_unwinding_context(): marking the unwound task {:?} as killed failed with error: {}", current_task, e);
                }
            }

            // (2) Remove it from its runqueue
            #[cfg(not(runqueue_state_spill_evaluation))]  // the normal case
            {
                if let Err(e) = apic::get_my_apic_id()
                    .and_then(|id| runqueue::get_runqueue(id))
                    .ok_or("couldn't get this core's ID or runqueue to remove killed task from it")
                    .and_then(|rq| rq.write().remove_task(&current_task)) 
                {
                    error!("BUG: cleanup_unwinding_context(): couldn't remove killed task from runqueue: {}", e);
                }
            }
        }

        // testing current task
        debug!("end of cleanup_unwinding_context(): curr_task is {:?}", task::get_my_current_task());

        // _held_interrupts are dropped here, which re-enables them (if initially enabled)
    }

    scheduler::schedule();

    error!("BUG: killed task was rescheduled! (in cleanup_unwinding_context())");
    loop { }
}
